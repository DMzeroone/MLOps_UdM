# ğŸš€ MÃ³dulo 04: Deployment de Modelos ML

Una guÃ­a completa sobre las **tres estrategias principales** para desplegar modelos de Machine Learning en producciÃ³n.

## ğŸ¯ Objetivo del MÃ³dulo

Aprender a llevar modelos ML desde el desarrollo hasta producciÃ³n usando diferentes estrategias segÃºn el caso de uso:

- **ğŸ”„ Batch Offline**: Predicciones programadas en lotes
- **ğŸŒ Online Web Service**: API REST para predicciones en tiempo real
- **âš¡ Online Streaming**: Procesamiento continuo de datos en tiempo real

## ğŸ“Š ComparaciÃ³n de Estrategias


| Aspecto             | Batch Offline | Web Service    | Streaming            |
| ------------------- | ------------- | -------------- | -------------------- |
| **â±ï¸ Latencia**   | Horas/DÃ­as   | Milisegundos   | Segundos             |
| **ğŸ“ˆ Volumen**      | Alto          | Medio          | Alto                 |
| **ğŸ’° Costo**        | Bajo          | Medio          | Alto                 |
| **ğŸ”§ Complejidad**  | Baja          | Media          | Alta                 |
| **ğŸ¯ Casos de Uso** | Reportes, ETL | Apps web, APIs | IoT, Fraud detection |

## 1ï¸âƒ£ Batch Offline Deployment

### ğŸ” Â¿QuÃ© es?

Procesamiento de **grandes volÃºmenes de datos** de forma programada, generalmente durante horas de baja actividad.

### ğŸ—ï¸ Arquitectura

```mermaid
graph TD
    A[ğŸ“Š Data Lake/Warehouse] --> B[ğŸ”„ Scheduler<br/>Cron/Airflow]
    B --> C[ğŸ¤– ML Model<br/>Batch Job]
    C --> D[ğŸ“ˆ Predictions]
    D --> E[ğŸ’¾ Results Storage<br/>Database/Files]
    E --> F[ğŸ“‹ Reports/Dashboard]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#e0f2f1
```

### ğŸ’¡ Casos de Uso Ideales

- **ğŸ“Š PredicciÃ³n de duraciÃ³n** para toda la flota de taxis
- **ğŸ’° EstimaciÃ³n de tarifas** para planificaciÃ³n de rutas
- **ğŸ“ˆ AnÃ¡lisis de patrones** de trÃ¡fico por zonas
- **ğŸ” OptimizaciÃ³n de rutas** basada en datos histÃ³ricos
- **ğŸ“§ Reportes operacionales** para compaÃ±Ã­as de taxi

### âš™ï¸ Flujo de Trabajo

```mermaid
sequenceDiagram
    participant S as ğŸ• Scheduler
    participant D as ğŸ“Š Data Source
    participant M as ğŸ¤– ML Model
    participant DB as ğŸ’¾ Database
    participant R as ğŸ“‹ Reports

    S->>D: 1. Extract data (daily/weekly)
    D->>M: 2. Load batch data
    M->>M: 3. Run predictions
    M->>DB: 4. Store results
    DB->>R: 5. Generate reports
    R->>S: 6. Send notifications
```

### ğŸ› ï¸ TecnologÃ­as Comunes

```python
# Ejemplo con Apache Airflow - PredicciÃ³n de DuraciÃ³n de Taxis NYC
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
import pandas as pd
import joblib
import numpy as np
from datetime import datetime, timedelta

def batch_taxi_duration_prediction():
    # Cargar modelo entrenado (del pipeline de Prefect)
    model = joblib.load('/models/taxi_duration_model.pkl')
    preprocessor = joblib.load('/models/preprocessor.b')

    # Cargar datos de viajes programados para maÃ±ana
    tomorrow = datetime.now() + timedelta(days=1)
    query = f"""
    SELECT trip_id, PULocationID, DOLocationID, trip_distance,
           pickup_datetime, passenger_count
    FROM scheduled_trips
    WHERE DATE(pickup_datetime) = '{tomorrow.date()}'
    """
    trips_data = pd.read_sql(query, connection)

    # Preparar features (mismo formato que en entrenamiento)
    trips_data['PU_DO'] = trips_data['PULocationID'].astype(str) + '_' + \
                         trips_data['DOLocationID'].astype(str)

    # Crear features usando el mismo preprocessor
    feature_dicts = trips_data[['PU_DO', 'trip_distance']].to_dict('records')
    X = preprocessor.transform(feature_dicts)

    # Predicciones de duraciÃ³n en minutos
    duration_predictions = model.predict(X)

    # Guardar resultados para planificaciÃ³n operacional
    results = pd.DataFrame({
        'trip_id': trips_data['trip_id'],
        'predicted_duration_minutes': duration_predictions,
        'estimated_fare': duration_predictions * 2.5,  # $2.5 por minuto
        'prediction_date': datetime.now(),
        'scheduled_pickup': trips_data['pickup_datetime']
    })

    # Guardar en base de datos para dashboard operacional
    results.to_sql('daily_trip_predictions', connection, if_exists='replace')

    print(f"Processed {len(results)} trip predictions for {tomorrow.date()}")
    print(f"Average predicted duration: {duration_predictions.mean():.1f} minutes")

# DAG para predicciones diarias
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'taxi_duration_batch_prediction',
    default_args=default_args,
    description='Daily batch prediction for NYC taxi trip durations',
    schedule_interval='0 2 * * *',  # 2 AM daily
    catchup=False
)

predict_task = PythonOperator(
    task_id='predict_taxi_durations',
    python_callable=batch_taxi_duration_prediction,
    dag=dag
)
```

### âœ… Ventajas

- **ğŸ’° Costo eficiente** para grandes volÃºmenes
- **ğŸ”§ ImplementaciÃ³n simple**
- **ğŸ“Š Ideal para anÃ¡lisis histÃ³ricos**
- **âš¡ OptimizaciÃ³n de recursos**

### âŒ Desventajas

- **â³ Alta latencia** (horas/dÃ­as)
- **ğŸš« No tiempo real**
- **ğŸ“… Dependiente de horarios**

## 2ï¸âƒ£ Online Web Service Deployment

### ğŸ” Â¿QuÃ© es?

**API REST** que recibe requests individuales y devuelve predicciones en **tiempo real** (milisegundos).

### ğŸ—ï¸ Arquitectura

```mermaid
graph TD
    A[ğŸ“± Client App<br/>Web/Mobile] --> B[ğŸŒ Load Balancer<br/>Nginx/AWS ALB]
    B --> C[ğŸ”„ API Gateway<br/>Rate Limiting]
    C --> D[ğŸ¤– ML Service<br/>Flask/FastAPI]
    D --> E[ğŸ’¾ Model Cache<br/>Redis/Memory]
    D --> F[ğŸ“Š Monitoring<br/>Prometheus]
    E --> G[ğŸ—„ï¸ Model Storage<br/>S3/MLflow]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#e0f2f1
    style G fill:#f9fbe7
```

### ğŸ’¡ Casos de Uso Ideales

- **ğŸš• EstimaciÃ³n de duraciÃ³n** en apps de taxi
- **ğŸ’° CÃ¡lculo de tarifas** dinÃ¡micas en tiempo real
- **ğŸ—ºï¸ OptimizaciÃ³n de rutas** para conductores
- **ğŸ“± Apps mÃ³viles** de transporte
- **ğŸ¯ AsignaciÃ³n inteligente** conductor-pasajero

### âš™ï¸ Flujo de Trabajo

```mermaid
sequenceDiagram
    participant C as ğŸ“± Client
    participant API as ğŸŒ API Gateway
    participant ML as ğŸ¤– ML Service
    participant Cache as ğŸ’¾ Cache
    participant DB as ğŸ—„ï¸ Database

    C->>API: 1. POST /predict
    API->>ML: 2. Forward request
    ML->>Cache: 3. Check model cache
    Cache->>ML: 4. Return cached model
    ML->>ML: 5. Run prediction
    ML->>DB: 6. Log request (async)
    ML->>API: 7. Return prediction
    API->>C: 8. JSON response
```

### ğŸ› ï¸ ImplementaciÃ³n con FastAPI

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import joblib
import numpy as np
from typing import Optional
import redis
import time
from datetime import datetime
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="NYC Taxi Duration Prediction API",
    description="API REST para predecir la duraciÃ³n de viajes de taxi en NYC usando XGBoost",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Cliente Redis para mÃ©tricas y cachÃ©
redis_client = redis.Redis(host='localhost', port=6379, db=0)

class TaxiTripRequest(BaseModel):
    """
    Modelo de request para predicciÃ³n de duraciÃ³n de viaje de taxi.

    Attributes:
        pickup_location_id (int): ID de la zona de recogida (1-263 para NYC)
        dropoff_location_id (int): ID de la zona de destino (1-263 para NYC)
        trip_distance (float): Distancia del viaje en millas
        passenger_count (int, optional): NÃºmero de pasajeros. Por defecto 1
        pickup_datetime (str, optional): Fecha/hora de recogida en formato ISO

    Example:
        {
            "pickup_location_id": 161,
            "dropoff_location_id": 236,
            "trip_distance": 2.5,
            "passenger_count": 2,
            "pickup_datetime": "2023-01-15T14:30:00"
        }
    """
    pickup_location_id: int = Field(
        ...,
        ge=1,
        le=263,
        description="ID de zona de recogida (1-263)"
    )
    dropoff_location_id: int = Field(
        ...,
        ge=1,
        le=263,
        description="ID de zona de destino (1-263)"
    )
    trip_distance: float = Field(
        ...,
        gt=0,
        le=100,
        description="Distancia del viaje en millas"
    )
    passenger_count: Optional[int] = Field(
        1,
        ge=1,
        le=6,
        description="NÃºmero de pasajeros"
    )
    pickup_datetime: Optional[str] = Field(
        None,
        description="Fecha/hora de recogida (ISO format)"
    )

class TaxiTripResponse(BaseModel):
    """
    Modelo de response con la predicciÃ³n de duraciÃ³n del viaje.

    Attributes:
        predicted_duration_minutes (float): DuraciÃ³n predicha en minutos
        estimated_fare (float): Tarifa estimada en USD
        confidence_score (float): Score de confianza (0.0-1.0)
        pickup_location (int): ID de zona de recogida
        dropoff_location (int): ID de zona de destino
        trip_distance (float): Distancia del viaje
        model_version (str): VersiÃ³n del modelo usado
        response_time_ms (int): Tiempo de respuesta en milisegundos
        prediction_timestamp (str): Timestamp de la predicciÃ³n
    """
    predicted_duration_minutes: float = Field(
        ...,
        description="DuraciÃ³n predicha del viaje en minutos"
    )
    estimated_fare: float = Field(
        ...,
        description="Tarifa estimada en USD"
    )
    confidence_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Score de confianza de la predicciÃ³n"
    )
    pickup_location: int = Field(..., description="ID de zona de recogida")
    dropoff_location: int = Field(..., description="ID de zona de destino")
    trip_distance: float = Field(..., description="Distancia del viaje en millas")
    model_version: str = Field(..., description="VersiÃ³n del modelo")
    response_time_ms: int = Field(..., description="Tiempo de respuesta en ms")
    prediction_timestamp: str = Field(..., description="Timestamp ISO de predicciÃ³n")

# Variables globales para modelo y preprocessor
model = None
preprocessor = None

@app.on_event("startup")
async def load_models():
    """
    Carga el modelo XGBoost y el preprocessor DictVectorizer al iniciar la aplicaciÃ³n.

    Raises:
        Exception: Si hay error cargando los modelos desde disco
    """
    global model, preprocessor
    try:
        logger.info("ğŸ”„ Cargando modelos...")
        model = joblib.load('/models/taxi_duration_model.pkl')
        preprocessor = joblib.load('/models/preprocessor.b')
        logger.info("âœ… Modelos cargados exitosamente")

        # Verificar que los modelos funcionan
        test_features = preprocessor.transform([{'PU_DO': '161_236', 'trip_distance': 2.5}])
        test_prediction = model.predict(test_features)
        logger.info(f"ğŸ§ª Test de modelo exitoso: {test_prediction[0]:.2f} minutos")

    except Exception as e:
        logger.error(f"âŒ Error cargando modelos: {e}")
        raise e

def calculate_confidence_score(trip_distance: float, pickup_id: int, dropoff_id: int) -> float:
    """
    Calcula un score de confianza para la predicciÃ³n basado en caracterÃ­sticas del viaje.

    Args:
        trip_distance (float): Distancia del viaje en millas
        pickup_id (int): ID de zona de recogida
        dropoff_id (int): ID de zona de destino

    Returns:
        float: Score de confianza entre 0.6 y 0.95

    Note:
        - Viajes muy largos (>50 millas) tienen menor confianza
        - Zonas conocidas (Manhattan central) tienen mayor confianza
    """
    # Base confidence
    confidence = 0.85

    # Penalizar viajes muy largos
    if trip_distance > 50:
        confidence -= 0.2
    elif trip_distance > 20:
        confidence -= 0.1

    # Bonus para zonas centrales de Manhattan (IDs comunes)
    manhattan_zones = [161, 236, 237, 238, 239, 140, 141, 142, 143]
    if pickup_id in manhattan_zones and dropoff_id in manhattan_zones:
        confidence += 0.1

    # Asegurar rango vÃ¡lido
    return round(max(0.6, min(0.95, confidence)), 3)

def calculate_estimated_fare(duration_minutes: float, trip_distance: float) -> float:
    """
    Calcula la tarifa estimada basada en duraciÃ³n y distancia.

    Args:
        duration_minutes (float): DuraciÃ³n predicha en minutos
        trip_distance (float): Distancia en millas

    Returns:
        float: Tarifa estimada en USD

    Note:
        FÃ³rmula: $2.50 base + $0.50/minuto + $2.50/milla + impuestos
    """
    base_fare = 2.50
    per_minute_rate = 0.50
    per_mile_rate = 2.50
    tax_rate = 0.50  # Impuesto fijo

    fare = base_fare + (duration_minutes * per_minute_rate) + (trip_distance * per_mile_rate) + tax_rate
    return round(fare, 2)

@app.post("/predict-duration", response_model=TaxiTripResponse)
async def predict_taxi_duration(request: TaxiTripRequest):
    """
    Predice la duraciÃ³n de un viaje de taxi en NYC.

    Args:
        request (TaxiTripRequest): Datos del viaje a predecir

    Returns:
        TaxiTripResponse: PredicciÃ³n con duraciÃ³n, tarifa y metadatos

    Raises:
        HTTPException: Error 500 si falla la predicciÃ³n

    Example:
        POST /predict-duration
        {
            "pickup_location_id": 161,
            "dropoff_location_id": 236,
            "trip_distance": 2.5
        }

        Response:
        {
            "predicted_duration_minutes": 12.34,
            "estimated_fare": 8.75,
            "confidence_score": 0.85,
            ...
        }
    """
    try:
        start_time = time.time()
        logger.info(f"ğŸš• PredicciÃ³n solicitada: {request.pickup_location_id} -> {request.dropoff_location_id}")

        # Crear feature PU_DO (mismo formato que en entrenamiento)
        pu_do = f"{request.pickup_location_id}_{request.dropoff_location_id}"

        # Preparar features para el modelo
        feature_dict = {
            'PU_DO': pu_do,
            'trip_distance': request.trip_distance
        }

        # Transformar usando el mismo preprocessor del entrenamiento
        X = preprocessor.transform([feature_dict])

        # PredicciÃ³n de duraciÃ³n en minutos
        duration_minutes = model.predict(X)[0]

        # Calcular tarifa estimada usando funciÃ³n dedicada
        estimated_fare = calculate_estimated_fare(duration_minutes, request.trip_distance)

        # Calcular confidence score
        confidence = calculate_confidence_score(
            request.trip_distance,
            request.pickup_location_id,
            request.dropoff_location_id
        )

        response_time = int((time.time() - start_time) * 1000)

        # MÃ©tricas para monitoreo (async)
        redis_client.incr("taxi_predictions_count")
        redis_client.incr(f"predictions_location_{request.pickup_location_id}")
        redis_client.lpush("recent_predictions", f"{duration_minutes:.2f}")
        redis_client.ltrim("recent_predictions", 0, 99)  # Mantener Ãºltimas 100

        logger.info(f"âœ… PredicciÃ³n completada: {duration_minutes:.2f} min, ${estimated_fare}")

        return TaxiTripResponse(
            predicted_duration_minutes=round(float(duration_minutes), 2),
            estimated_fare=estimated_fare,
            confidence_score=confidence,
            pickup_location=request.pickup_location_id,
            dropoff_location=request.dropoff_location_id,
            trip_distance=request.trip_distance,
            model_version="v1.0",
            response_time_ms=response_time,
            prediction_timestamp=datetime.utcnow().isoformat()
        )

    except Exception as e:
        logger.error(f"âŒ Error en predicciÃ³n: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error en predicciÃ³n: {str(e)}")

@app.get("/health")
async def health_check():
    """
    Endpoint de health check para verificar el estado del servicio.

    Returns:
        dict: Estado del servicio, modelos cargados y mÃ©tricas bÃ¡sicas

    Example:
        GET /health

        Response:
        {
            "status": "healthy",
            "model_loaded": true,
            "preprocessor_loaded": true,
            "total_predictions": 1234,
            "uptime_seconds": 3600
        }
    """
    try:
        total_predictions = redis_client.get("taxi_predictions_count")
        total_predictions = int(total_predictions) if total_predictions else 0

        return {
            "status": "healthy",
            "model_loaded": model is not None,
            "preprocessor_loaded": preprocessor is not None,
            "total_predictions": total_predictions,
            "redis_connected": redis_client.ping(),
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/model-info")
async def model_info():
    """
    InformaciÃ³n detallada sobre el modelo ML en uso.

    Returns:
        dict: Metadatos del modelo, features, datos de entrenamiento y performance

    Example:
        GET /model-info

        Response:
        {
            "model_type": "XGBoost Regressor",
            "target": "trip_duration_minutes",
            "features": ["PU_DO", "trip_distance"],
            "training_data": "NYC Taxi 2023-01, 2023-02",
            "expected_rmse": "~5.2 minutes",
            "version": "v1.0"
        }
    """
    return {
        "model_type": "XGBoost Regressor",
        "target": "trip_duration_minutes",
        "features": ["PU_DO", "trip_distance"],
        "feature_count": "~5700 location pairs",
        "training_data": "NYC Taxi 2023-01 (train), 2023-02 (validation)",
        "training_samples": "~133K trips",
        "expected_rmse": "~5.2 minutes",
        "model_size": "~2MB",
        "version": "v1.0",
        "last_updated": "2023-03-01",
        "preprocessing": "DictVectorizer for categorical encoding"
    }

@app.get("/metrics")
async def get_metrics():
    """
    MÃ©tricas operacionales del servicio.

    Returns:
        dict: MÃ©tricas de uso, performance y estadÃ­sticas de predicciones
    """
    try:
        total_predictions = redis_client.get("taxi_predictions_count")
        total_predictions = int(total_predictions) if total_predictions else 0

        # Obtener Ãºltimas predicciones para estadÃ­sticas
        recent_predictions = redis_client.lrange("recent_predictions", 0, -1)
        recent_durations = [float(p.decode()) for p in recent_predictions]

        avg_duration = sum(recent_durations) / len(recent_durations) if recent_durations else 0

        return {
            "total_predictions": total_predictions,
            "recent_predictions_count": len(recent_durations),
            "avg_predicted_duration_minutes": round(avg_duration, 2),
            "min_duration": round(min(recent_durations), 2) if recent_durations else 0,
            "max_duration": round(max(recent_durations), 2) if recent_durations else 0,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ Error getting metrics: {e}")
        return {"error": str(e)}
```

### ğŸ³ ContainerizaciÃ³n con Docker

```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### â˜¸ï¸ Deployment con Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-api
  template:
    metadata:
      labels:
        app: ml-api
    spec:
      containers:
      - name: ml-api
        image: ml-api:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: ml-api-service
spec:
  selector:
    app: ml-api
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### âœ… Ventajas

- **âš¡ Baja latencia** (< 100ms)
- **ğŸ”„ Tiempo real**
- **ğŸ“ˆ Escalable horizontalmente**
- **ğŸ”§ FÃ¡cil integraciÃ³n** con apps

### âŒ Desventajas

- **ğŸ’° MÃ¡s costoso** que batch
- **ğŸ”§ Mayor complejidad** operacional
- **ğŸ“Š Requiere monitoreo** constante

## 3ï¸âƒ£ Online Streaming Deployment

### ğŸ” Â¿QuÃ© es?

Procesamiento **continuo** de flujos de datos en tiempo real, ideal para eventos que requieren **respuesta inmediata**.

### ğŸ—ï¸ Arquitectura

```mermaid
graph TD
    A[ğŸ“¡ Data Sources<br/>IoT/Logs/Events] --> B[ğŸ“¨ Message Broker<br/>Kafka/Pulsar]
    B --> C[âš¡ Stream Processor<br/>Kafka Streams/Flink]
    C --> D[ğŸ¤– ML Model<br/>Online Learning]
    D --> E[ğŸ“Š Real-time Analytics<br/>InfluxDB/Elasticsearch]
    D --> F[ğŸš¨ Alert System<br/>PagerDuty/Slack]
    D --> G[ğŸ’¾ Data Sink<br/>Database/S3]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#ffebee
    style G fill:#e0f2f1
```

### ğŸ’¡ Casos de Uso Ideales

- **ğŸš• Ajuste dinÃ¡mico de tarifas** en tiempo real
- **ğŸ“Š Monitoreo de trÃ¡fico** y congestiÃ³n
- **ğŸ¯ AsignaciÃ³n inteligente** conductor-pasajero
- **ğŸ—ºï¸ OptimizaciÃ³n de rutas** en vivo
- **ğŸ“ˆ PredicciÃ³n de demanda** por zona
- **ğŸš¦ GestiÃ³n de flota** en tiempo real

### âš™ï¸ Flujo de Trabajo

```mermaid
sequenceDiagram
    participant S as ğŸ“¡ Data Source
    participant K as ğŸ“¨ Kafka
    participant P as âš¡ Processor
    participant ML as ğŸ¤– ML Model
    participant A as ğŸš¨ Alerts
    participant DB as ğŸ’¾ Storage

    S->>K: 1. Stream events
    K->>P: 2. Consume messages
    P->>ML: 3. Feature extraction
    ML->>ML: 4. Real-time prediction
    ML->>A: 5. Trigger alerts (if needed)
    ML->>DB: 6. Store results
    P->>K: 7. Produce enriched events
```

### ğŸ› ï¸ ImplementaciÃ³n con Kafka Streams

```python
from kafka import KafkaConsumer, KafkaProducer
import json
import joblib
import numpy as np
from datetime import datetime
import logging

class TaxiDurationStreamProcessor:
    def __init__(self):
        # Cargar modelo y preprocessor entrenados
        self.model = joblib.load('/models/taxi_duration_model.pkl')
        self.preprocessor = joblib.load('/models/preprocessor.b')

        # Configurar Kafka consumer para viajes en tiempo real
        self.consumer = KafkaConsumer(
            'taxi_trip_requests',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )

        # Producer para enviar predicciones
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def extract_features(self, trip_request):
        """Extrae features del request de viaje"""
        # Crear PU_DO feature (mismo formato que entrenamiento)
        pu_do = f"{trip_request['pickup_location_id']}_{trip_request['dropoff_location_id']}"

        feature_dict = {
            'PU_DO': pu_do,
            'trip_distance': trip_request['trip_distance']
        }

        # Usar el mismo preprocessor del entrenamiento
        return self.preprocessor.transform([feature_dict])

    def calculate_dynamic_fare(self, duration_minutes, base_distance, surge_multiplier=1.0):
        """Calcula tarifa dinÃ¡mica basada en duraciÃ³n predicha"""
        base_fare = 2.5
        per_minute_rate = 0.5
        per_mile_rate = 2.0

        fare = (base_fare +
                (duration_minutes * per_minute_rate) +
                (base_distance * per_mile_rate)) * surge_multiplier

        return round(fare, 2)

    def process_stream(self):
        """Procesa el stream de requests de taxi en tiempo real"""
        self.logger.info("ğŸš• Starting taxi duration stream processor...")

        for message in self.consumer:
            try:
                trip_request = message.value
                start_time = datetime.utcnow()

                # Extraer features
                features = self.extract_features(trip_request)

                # PredicciÃ³n de duraciÃ³n
                predicted_duration = self.model.predict(features)[0]

                # Determinar si es viaje largo (>30 min) para alertas
                is_long_trip = predicted_duration > 30

                # Calcular tarifa dinÃ¡mica
                surge_multiplier = trip_request.get('surge_multiplier', 1.0)
                estimated_fare = self.calculate_dynamic_fare(
                    predicted_duration,
                    trip_request['trip_distance'],
                    surge_multiplier
                )

                # Enriquecer evento con predicciÃ³n
                enriched_trip = {
                    **trip_request,
                    'predicted_duration_minutes': round(float(predicted_duration), 2),
                    'estimated_fare': estimated_fare,
                    'is_long_trip': is_long_trip,
                    'processing_time_ms': int((datetime.utcnow() - start_time).total_seconds() * 1000),
                    'prediction_timestamp': datetime.utcnow().isoformat(),
                    'model_version': 'v1.0'
                }

                # Enviar a diferentes topics segÃºn tipo de viaje
                if is_long_trip:
                    # Alertar para viajes largos (>30 min)
                    self.producer.send('long_trip_alerts', enriched_trip)
                    self.logger.warning(
                        f"LONG TRIP: {trip_request['trip_id']} - "
                        f"{predicted_duration:.1f} min predicted"
                    )

                # Enviar predicciÃ³n para asignaciÃ³n de conductor
                self.producer.send('trip_duration_predictions', enriched_trip)

                # Log para monitoreo
                self.logger.info(
                    f"Processed trip {trip_request['trip_id']}: "
                    f"{predicted_duration:.1f} min, ${estimated_fare}"
                )

            except Exception as e:
                self.logger.error(f"Error processing trip request: {e}")
                # Enviar a topic de errores para debugging
                error_event = {
                    'original_message': message.value,
                    'error': str(e),
                    'timestamp': datetime.utcnow().isoformat()
                }
                self.producer.send('processing_errors', error_event)

# Ejecutar el stream processor
if __name__ == "__main__":
    processor = TaxiDurationStreamProcessor()
    processor.process_stream()
```

### ğŸ”„ Apache Flink Implementation

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
import joblib

def fraud_detection_job():
    # Configurar entorno
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(4)

    # Kafka source
    kafka_source = FlinkKafkaConsumer(
        topics=['transactions'],
        deserialization_schema=JsonRowDeserializationSchema.builder()
            .type_info(Types.ROW([
                Types.STRING(),  # transaction_id
                Types.DOUBLE(),  # amount
                Types.INT(),     # merchant_category
                Types.LONG()     # timestamp
            ])).build(),
        properties={'bootstrap.servers': 'localhost:9092'}
    )

    # Stream processing
    stream = env.add_source(kafka_source)

    # Aplicar modelo ML
    predictions = stream.map(lambda transaction: {
        'transaction_id': transaction[0],
        'fraud_score': predict_fraud(transaction),
        'timestamp': transaction[3]
    })

    # Sink para alertas
    predictions.filter(lambda x: x['fraud_score'] > 0.7) \
              .add_sink(kafka_fraud_alerts_sink)

    env.execute("Fraud Detection Stream")

def predict_fraud(transaction):
    # Cargar modelo (cache en memoria)
    model = get_cached_model()
    features = extract_features(transaction)
    return model.predict_proba([features])[0][1]
```

### ğŸ“Š Monitoreo y MÃ©tricas

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# MÃ©tricas de monitoreo
TRIPS_PROCESSED = Counter('trips_processed_total', 'Total taxi trips processed')
LONG_TRIPS_DETECTED = Counter('long_trips_detected_total', 'Total long trips detected (>30min)')
PROCESSING_TIME = Histogram('processing_time_seconds', 'Time spent processing trip request')
AVG_PREDICTED_DURATION = Gauge('avg_predicted_duration_minutes', 'Average predicted trip duration')
MODEL_RMSE = Gauge('model_rmse', 'Current model RMSE in minutes')

def process_with_monitoring(trip_request):
    start_time = time.time()

    try:
        # Procesar request de viaje
        result = process_trip_request(trip_request)

        # Actualizar mÃ©tricas
        TRIPS_PROCESSED.inc()
        if result['predicted_duration_minutes'] > 30:
            LONG_TRIPS_DETECTED.inc()

        # Actualizar duraciÃ³n promedio
        AVG_PREDICTED_DURATION.set(result['predicted_duration_minutes'])

    finally:
        PROCESSING_TIME.observe(time.time() - start_time)
```

### âœ… Ventajas

- **âš¡ Ultra baja latencia** (milisegundos)
- **ğŸ”„ Procesamiento continuo**
- **ğŸ“ˆ Escalabilidad masiva**
- **ğŸ¯ Respuesta inmediata** a eventos crÃ­ticos

### âŒ Desventajas

- **ğŸ”§ Alta complejidad** tÃ©cnica
- **ğŸ’° Costoso** de mantener
- **ğŸ› ï¸ Requiere expertise** especializado
- **ğŸ“Š Monitoreo complejo**

## ğŸ”„ Patrones de Deployment HÃ­bridos

### ğŸ¯ Lambda Architecture

Combina **batch** y **streaming** para obtener lo mejor de ambos mundos:

```mermaid
graph TD
    A[ğŸ“Š Data Sources] --> B[ğŸ“¨ Message Queue]
    A --> C[ğŸ’¾ Data Lake]

    B --> D[âš¡ Speed Layer<br/>Real-time Stream]
    C --> E[ğŸ”„ Batch Layer<br/>Historical Processing]

    D --> F[ğŸ“ˆ Serving Layer<br/>Combined Results]
    E --> F

    F --> G[ğŸ“± Applications]

    style D fill:#ffebee
    style E fill:#e8f5e8
    style F fill:#fff3e0
```

### ğŸ”„ Kappa Architecture

Solo **streaming**, pero con capacidad de reprocesar datos histÃ³ricos:

```mermaid
graph TD
    A[ğŸ“Š All Data as Stream] --> B[âš¡ Stream Processor]
    B --> C[ğŸ’¾ Immutable Log]
    C --> D[ğŸ“ˆ Materialized Views]
    D --> E[ğŸ“± Applications]

    C --> F[ğŸ”„ Reprocessing<br/>for Model Updates]
    F --> B
```

---

## ğŸ› ï¸ Herramientas y TecnologÃ­as

### ğŸ“¦ Batch Processing

- **Apache Airflow**: OrquestaciÃ³n de workflows
- **Apache Spark**: Procesamiento distribuido
- **Prefect**: OrquestaciÃ³n moderna
- **Kubeflow**: ML pipelines en Kubernetes

### ğŸŒ Web Services

- **FastAPI/Flask**: APIs Python
- **Docker**: ContainerizaciÃ³n
- **Kubernetes**: OrquestaciÃ³n de containers
- **AWS Lambda**: Serverless functions

### âš¡ Stream Processing

- **Apache Kafka**: Message broker
- **Apache Flink**: Stream processing
- **Kafka Streams**: LibrerÃ­a Java/Scala
- **Apache Pulsar**: Message broker moderno

### ğŸ“Š Monitoreo

- **Prometheus + Grafana**: MÃ©tricas y dashboards
- **ELK Stack**: Logs centralizados
- **MLflow**: Model tracking
- **Evidently AI**: Model monitoring

## ğŸ¯ GuÃ­a de DecisiÃ³n

### â“ Â¿QuÃ© estrategia elegir?

```mermaid
flowchart TD
    A[Â¿Necesitas respuesta inmediata?] -->|SÃ­| B[Â¿Volumen muy alto?]
    A -->|No| C[Â¿Procesamiento programado?]

    B -->|SÃ­| D[ğŸ”„ Streaming]
    B -->|No| E[ğŸŒ Web Service]

    C -->|SÃ­| F[ğŸ“Š Batch Offline]
    C -->|No| G[Â¿InteracciÃ³n de usuario?]

    G -->|SÃ­| E
    G -->|No| F

    style D fill:#ffebee
    style E fill:#e8f5e8
    style F fill:#e1f5fe
```

### ğŸ“‹ Checklist de DecisiÃ³n

**Usa Batch cuando:**

- âœ… Latencia > 1 hora es aceptable
- âœ… Grandes volÃºmenes de datos
- âœ… Procesamiento programado
- âœ… Costos son crÃ­ticos

**Usa Web Service cuando:**

- âœ… Latencia < 1 segundo requerida
- âœ… InteracciÃ³n directa con usuarios
- âœ… Volumen moderado
- âœ… IntegraciÃ³n con aplicaciones

**Usa Streaming cuando:**

- âœ… Latencia < 100ms crÃ­tica
- âœ… Eventos continuos
- âœ… Decisiones en tiempo real
- âœ… Tolerancia a alta complejidad

## ğŸ“š Recursos Adicionales

### ğŸ“– Lecturas Recomendadas

- [Designing Data-Intensive Applications](https://dataintensive.net/)
- [Building Machine Learning Powered Applications](https://mlpowered.com/)
- [ML Engineering at Scale](https://www.oreilly.com/library/view/ml-engineering-at/9781617298196/)

### ğŸ“ Cursos

- [MLOps Specialization - Coursera](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)
- [Kafka Streams - Confluent](https://developer.confluent.io/learn-kafka/)

### ğŸ› ï¸ Herramientas para Practicar

- [MLflow](https://mlflow.org/)
- [Kubeflow](https://www.kubeflow.org/)
- [Apache Airflow](https://airflow.apache.org/)
- [Kafka](https://kafka.apache.org/)

## ğŸ¯ PrÃ³ximos Pasos

1. **ğŸ”¬ Experimenta** con cada tipo de deployment
2. **ğŸ“Š Compara** rendimiento y costos
3. **ğŸ› ï¸ Implementa** monitoreo desde el dÃ­a 1
4. **ğŸ“ˆ Escala** gradualmente segÃºn necesidades
5. **ğŸ”„ Itera** basÃ¡ndote en feedback de producciÃ³n

*Â¿Listo para llevar tus modelos a producciÃ³n? Â¡Elige la estrategia que mejor se adapte a tu caso de uso!*
